Cross-Entropy(交叉熵)
==

### Entropy 
原本是用在表達化學平衡態的名詞，藉以表達原子在空間當中的分布狀況，如果原子分佈越混亂，我們則說**Entropy越高，表示資訊量越高，越多不確定性存在**，因為有更多因素去影響我們原子的分布狀況。
>例1 : 假設隨機從一個口袋裡取硬幣，口袋裡有一個藍色的，一個紅色的，一個綠色的，一個橘色的。取出一個硬幣之後，每次問一個問題，然後做出判斷，目標是，問最少的問題，得到正確答案。其中一個最好的策略如下 : 相問是不是紅或藍，分為兩組後，再問是否為紅及是否為橘，就能找出所有的球，由於問了兩道題目，$1/4(機率) * 2道題目 * 4顆球 = 2$，平均需要問$2$道題目才能找出不同顏色的球，也就是說期望值為$2$，代表**entropy**。

>例2 : 例1中變成了袋子中$\dfrac{1}{8}$硬幣是綠色的，$\dfrac{1}{8}$的是橘色的，$\dfrac{1}{4}$是紅色的，$\dfrac{1}{2}$是藍色的，這時最優的策略如下 : 先問是否為藍；再問是否為紅；最後問是否為橘。其中，$\dfrac{1}{2}$ 的概率是藍色，只需要$1$個問題就可以知道是或者不是，$\dfrac{1}{4}$ 的概率是紅色，需要$2$個問題，按照這個邏輯，猜中硬幣需要的問題的期望值是 $\dfrac{1}{2}*1 + \dfrac{1}{4}*2 + \dfrac{1}{8}*3 + \dfrac{1}{8}*3 = 1.75$

>總結上面的例子，假設一種硬幣出現的概率是$p$，那麼猜中該硬幣的所需要的問題數是$\log_2\dfrac{1}{p}$

>公式 : (問題個數的期望值，其意義就是在最優化策略下，猜到顏色所需要的問題的個數，不確定性越高，entropy就越大)
![](https://i.imgur.com/HchuUXc.png)

### Cross-entropy
用意是在觀測預測的機率分佈與實際機率分布的誤差範圍，就拿下圖為例就直覺說明，cross-entropy(purple line = area under the blue curve)，我們預測的機率分佈為橘色區塊，真實的機率分佈為紅色區塊，藍色的地方就是cross-entropy區塊，紫色現為計算出來的值。我們預測值與實際值差越多，也就是代表內涵的資訊量愈大，也就是不確定越多，也就是 cross-entropy 會越高。
![](https://i.imgur.com/XEcuOch.png)
反之，如果疊合的區塊越多，就代表不確定性越少，也就是 cross-entropy 會越小，如下圖所示。
![](https://i.imgur.com/rUwjg56.png)

>例 : 如果在例2中使用例1的策略 : $\dfrac{1}{8}$的概率硬幣是橘色，需要$2$個問題，$\dfrac{1}{2}$的概率是藍色，仍然需要$2$個問題，平均來說，需要的問題數是 $\dfrac{1}{8}*2 + \dfrac{1}{8}*2 + \dfrac{1}{4}*2 + \dfrac{1}{2}*2 = 2$。因此，在例2中使用例1的策略是一個比較差的策略。其中2是這個方案中的cross-entropy。

>因此，給定一個策略，cross-entropy就是在該策略下猜中顏色所需要的問題的期望值，越好的策略，最終的cross-entropy越低。具有最低的cross-entropy的策略就是最優化策略。也就是上面定義的entropy。因此，在機器學習中，我們需要最小化cross-entropy。

>公式 : (p為實際機率分布，q為預測機率分布)
![](https://i.imgur.com/yLgynrk.png)
